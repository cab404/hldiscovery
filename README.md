# hldiscovery

Here you can find algorithm implementation as a set of __PM4Py__ using modules.

In **tests** folder there are initial raw logs for the large model from the paper and resulting logs and Petri nets in **final logs** and **final nets** folders respectively.

To check conformance of resulting logs with initially created model (with domain knowledge of expert) we need to replay resulting traces on the model. For each trace, there are four values which have to be determined: **p**roduced tokens, **r**emaining tokens, **m**issing tokens, and **c**onsumed tokens. Based on that, a fomrula can be dervied, whereby a petri net $N$ and a trace $σ$ are given as input:

$$ fitness(σ,N) = 1/2*(1-m/c)+1/2*(1-r/p) $$

So in our case with cycles the value of the $fitness$ indicator is directly related to the coverage of the initial logs, for small logs - the coverage of possible model behaviour is quite limited, which directly affects the ability of the algorithm to correctly extract information about cycles.
For 90 initial logs (10 traces in each) from the **initial logs** folder the fitness measures are next:

1.0

0.8554464285714286

0.9373861885418127

0.7518122247158647

0.9048654365965448

0.7691866040587554

0.8696507624200689

0.9601142615319509

0.8893280632411067

0.8573846573846574

1.0

0.9146612435506346

0.7969617680477539

0.8548172757475083

0.8067978533094813

0.831077694235589

0.7994834988242058

1.0

0.7795775314956901

1.0

0.9003159358018451

0.9344332855093256

0.8687836553690211

0.8167950693374422

0.8052198092520673

0.790516152189519

0.9373511367737279

0.9022029065489331

0.9359340659340659

1.0

1.0

0.8828231292517006

0.9063841573189308

0.7826257728379533

0.9079595540575577

0.9019810508182602

0.8132961675924342

0.7849556234540958

0.7378193632757587

0.8126743256158693

0.8556769281694803

0.8502909039307507

0.7206760707593037

0.8199912701876909

0.8629239766081871

0.8962452453583399

0.8617363608005385

0.758680587204149

0.82557781201849

0.8480281578004538

0.980857310628303

0.8212370005473453

0.7809412721729396

0.7941427076209225

0.8795955882352942

0.769164924967805

0.7751168661365847

0.7323750962942823

0.751033342838001

0.7737558193357181

0.8104331625523986

0.8962738219398881

0.8056078767123288

0.8389304812834224

0.6651124989503543

0.8085330059549009

0.794975200482084

0.8123154667272314

0.9408158158158157

0.9068228442856198

0.8113204038677617

0.9063841573189308

0.9066141336239417

0.9019810508182602

1.0

1.0

0.9571895424836601

0.8946078431372549

0.854616523434679

0.779826585248101

0.827458520259204

0.8475575662074926

0.9580191050779285

0.7988533205924511

0.8416777926123721

0.7770379220262019

0.8281481481481481

0.8760774040899104

0.9094911937377691

0.9125371089289792
